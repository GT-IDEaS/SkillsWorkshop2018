{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and Machine Learning\n",
    "Today we will be learning about the popular machine learning API, TensorFlow. \n",
    "<br>  - Primarily for deep neural networks.\n",
    "<br>  - Developed by the Google Brain team.\n",
    "<br>  - Allows for rapid construction and efficient processing.\n",
    "<br>  - Can run on multiple CPUs and GPUs.\n",
    "<br>  - Primary benefit: The user no longer has to explicitly code gradient computations (backpropagation in NN) used for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import numpy, just in case.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the tensorflow library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset the default graph just in case something wonky is happening\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensorflow Graph \n",
    "<br>  - Every opertaion in your model is added to the graph.\n",
    "<br>  - The graph is constructed before any numerical computing occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph object\n",
    "graph = None\n",
    "print(graph.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a matrix, set the matrix as a Tensorflow constant (numerical object with Tensorflow class \"Tensor\")\n",
    "a = np.random.randn(2,2)\n",
    "A = tf.constant(a)\n",
    "print('Numpy a: \\n', a)\n",
    "print('Tensorflow A: \\n ', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While 'a' is in memory as a numpy array with numerical values, 'A' is stored as an object. <br> To retrieve these values we must run the variable in a 'session'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the standard syntax for creating a session object to run parts of our graph. \n",
    "sess = None\n",
    "# Use the session object to view A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try running the cell again, note that the values do not change. 'A' is referencing already generated values of 'a' stored in memory via the numpy function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run this cell a few times. What changes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Tensorflow for this...\n",
    "B = None\n",
    "print('Without running via a session: \\n', B)\n",
    "print('With a session, \\n', sess.run(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try printing the graph operations again.\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A key takeaway here is that Tensorflow has a lot going on 'under the hood' to keep track of every operation added to the graph and that tf.Tensor does not return a value outside of a session.\n",
    "\n",
    "### Next, we will aim to apply tensorflow to run a logistic regression on a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification via ML Algortihms\n",
    "Today we will focus on two types of machine learning methods to perform binary classification. \n",
    "<br> - Logistic Regression \n",
    "<br> - Simple, 1 layer Neural Network\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Objective: Map data from input $X$ (n features, m examples) to output $Y$ (vector with values 0 or 1 for each training example). \n",
    "\n",
    "Accomplished using the sigmoid function \n",
    "## $\\sigma(Z) = \\frac{1}{1 + e^{-Z}}$\n",
    "\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Logistic-curve.svg#/media/File:Logistic-curve.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" alt=\"Logistic-curve.svg\" height=\"145\" width=\"218\"></a><br>By <a href=\"//commons.wikimedia.org/wiki/User:Qef\" title=\"User:Qef\">Qef</a> (<a href=\"//commons.wikimedia.org/wiki/User_talk:Qef\" title=\"User talk:Qef\">talk</a>) - Created from scratch with gnuplot, Public Domain, <a href=\"https://commons.wikimedia.org/w/index.php?curid=4310325\">Link</a></p>\n",
    "\n",
    "<br> To build this model we will first need to generate $Z$ from $X$ and then use the activation $\\sigma(Z) == \\hat{Y}$ which is out output array of predicted binary class probabilities.\n",
    "## $Z = W^{T}X + b $ \n",
    "<br> Where $W$ is a vector of weights and $b$ is a bias term.\n",
    "## $\\hat{Y} = \\sigma(Z)$ \n",
    "__NOTE__: Later we will refer to $\\sigma()$ as a general activation function $g()$ \n",
    "\n",
    "Without excessive motivation, the cost or objective function over the whole training set $X$ can be defined as:\n",
    "\n",
    "## $ J(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}[-y_i\\log{(\\hat{y_i})} - (1-y_i)\\log{(1-\\hat{y_i})}]$\n",
    "\n",
    "Which is known as the log-loss function\n",
    "<br>Note: We have defined a non-linear function that must use an optimization technique to solve for these parameters.\n",
    "Because the gradient of the cost $J$ can be defined, it is standard practice to solve for the gradient of the cost with respect to $W$ and $b$ and use this information with a gradient descent optimizer.\n",
    "<br> This is accomplished through what I will go ahead and refer to as backpropagation which is machine learning terminology for defining a computation gradient of the cost function using the __chain rule__ from calculus.\n",
    "\n",
    "<br> This is a very tedious process which I reccomend you explore in a machine learning course if you desire. Luckily for us today, we will introduce Tensorflow, a graph based machine learning API that keeps track of our calculations and computes the gradients of the parameters for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load the data set for the day\n",
    "\n",
    "data = pd.read_csv('tfdata.csv', header = None)\n",
    "X = np.array(data[[0,1]])\n",
    "X = X.T\n",
    "n, m = X.shape\n",
    "y = np.array(data[2]).reshape(1, m)\n",
    "\n",
    "# Define a plot fn to view the planar data we wish to classify\n",
    "def plotdata(X, y):\n",
    "    y = np.squeeze(y)\n",
    "    fig = plt.figure(num = 1)\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for l, c, m in zip(range(2), ('blue', 'red'), ('^', 's')):\n",
    "        ax1.scatter(X[0,:][y == l], X[1,:][y == l],\n",
    "            color=c,\n",
    "            label='class %s' % l,\n",
    "            alpha=0.5,\n",
    "            marker=m)\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x2')\n",
    "    ax1.set_title('Sample Plot')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()\n",
    "\n",
    "plotdata(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's clear the graph to build our model.\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LRmodel(X, W, b):\n",
    "    # X is n features x m examples input matrix\n",
    "    # W is (1,n features) weights array\n",
    "    # b.shape = (1, 1) bias value.\n",
    "    # Function returns the sigmoid of Z\n",
    "    \n",
    "    Z = None\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a model function, it has not been added to the graph yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LRtrain(X_train, Y_train, iterations = 100, learning_rate = 0.001, thresh = 0.75):\n",
    "    # X should be passed to the function as a n x m matrix\n",
    "    # Y should be 1 x m\n",
    "    \n",
    "    n, m = X_train.shape\n",
    "    \n",
    "    # We define a placeholder with a variable amount of training examples for the purpose of buidling the graph.\n",
    "    X = tf.placeholder()\n",
    "    Y = tf.placeholder()\n",
    "    \n",
    "    # Declare our parameters\n",
    "    W = tf.get_variable(str(None), [None, None], initializer= tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(str(None), [None, None], initializer= tf.zeros_initializer())\n",
    "    \n",
    "    \n",
    "    # Obtain y_hat or model for class probabilities\n",
    "    y_hat = None\n",
    "    \n",
    "    \n",
    "    # Define a cost function\n",
    "    cost = tf.losses.log_loss(Y, y_hat)\n",
    "    \n",
    "    # Define an optimeizer object\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Define a weight initializer object\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Run update parameters over a preset number of iterations (for the class demo purposes only)\n",
    "        for i in range(iterations):\n",
    "            _, err = sess.run([None, None], feed_dict= {X:None, Y:None})\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(err)\n",
    "        \n",
    "        # Define an accuracy protocol\n",
    "        prediction = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "        evaluate_predict = tf.equal(prediction, Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(evaluate_predict, \"float\"))\n",
    "\n",
    "        print('Logistic regression training set accuracy = ', accuracy.eval({X:None, Y:None}))\n",
    "        # Include a printout of the scatter plot and decision boundary\n",
    "        \n",
    "    return W, b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logistic regression training step and see how we do.\n",
    "tf.reset_default_graph()\n",
    "W, b = LRtrain(X, y, iterations=10000, learning_rate= 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "To extend the ideas of logistic regression, we will now add in a \"hidden\" layer, expanding or contracting our input dimension to a different space. Let's examine this graphically:\n",
    "## Before with logistic regression,\n",
    "\n",
    "![LR Figure](lrfig.png)\n",
    "\n",
    "## Now, with a single layer added,\n",
    "\n",
    "![NN Figure](nnfig.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For a simple 1 layer neural network:\n",
    "\n",
    "<br> $ a^{[1]} = g^{[1]}(W^{[1]}X + b^{[1]}) $\n",
    "<br> $ a^{[2]} = g^{[2]}(W^{[2]}a^{[1]} + b^{[2]}) $    Where $ a^{[2]} $ is our output activation/class probability and $g^{[2]}$ is a sigmoid function.\n",
    "\n",
    "### To start, define the paramters to be initialized in a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_params(n_feat, layer_size):\n",
    "    \n",
    "    # n_feat is the number of features in the sample\n",
    "    # layer_size is the size of layer 1, the only layer in the NN\n",
    "    params = {}\n",
    "    params['W1'] = tf.get_variable('W1', [None, None], initializer= tf.contrib.layers.xavier_initializer())\n",
    "    params['b1'] = tf.get_variable('b1', [None, None], initializer= tf.zeros_initializer())\n",
    "    params['W2'] = tf.get_variable('W2', [None, None], initializer= tf.contrib.layers.xavier_initializer())\n",
    "    params['b2'] = tf.get_variable('b2', [None, None], initializer= tf.zeros_initializer())\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the model we are using or in deep learning terms, define the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NNmodel(X, params):\n",
    "    # X is n_feat x m examples\n",
    "    # params = a dict containing the parameters we previously deifned.\n",
    "    \n",
    "    \n",
    "    # Retrieve the parameters\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    # Now implement the forward propagation using tensorflow\n",
    "    a1 = None\n",
    "    a2 = None\n",
    "    \n",
    "    return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Let's define a training function similar to before to train our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NNtrain(X_train, Y_train, layer_size = 10, iterations = 100, learning_rate = 0.001, thresh = 0.75):\n",
    "    # X should be passed to the function as a n x m matrix\n",
    "    # Y should be 1 x m\n",
    "    \n",
    "    n, m = X_train.shape\n",
    "    \n",
    "    # We define a placeholder with a variable amount of training examples for the purpose of buidling the graph.\n",
    "    X = tf.placeholder(shape = (n, None), dtype = tf.float32)\n",
    "    Y = tf.placeholder(shape = (1, None), dtype = tf.float32)\n",
    "    \n",
    "    # add our model parameters to the graph variables\n",
    "    params = add_params(n, layer_size)\n",
    "    \n",
    "    \n",
    "    # Define our forward propagation model\n",
    "    y_hat = NNmodel(X, params)\n",
    "    \n",
    "    \n",
    "    # Define a cost function\n",
    "    cost = tf.losses.log_loss(Y, y_hat)\n",
    "    \n",
    "    # Define an optimeizer object\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    # Define a weight initializer object\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Run update parameters over a preset number of iterations (for the class demo purposes only)\n",
    "        for i in range(iterations):\n",
    "            _, err = sess.run([optimizer, cost], feed_dict= {X:X_train, Y:Y_train})\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(err)\n",
    "        \n",
    "        prediction = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "        evaluate_predict = tf.equal(prediction, Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(evaluate_predict, \"float\"))\n",
    "        print('Neural network training set accuracy = ', accuracy.eval({X:X_train, Y:Y_train}))\n",
    "        # Include a printout of the scatter plot and decision boundary\n",
    "        \n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "params = NNtrain(X, y, layer_size= 10, iterations = 10000, learning_rate= 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Wow! That's quite an improvement for only adding one layer with a few hidden units. As the amount of layers and units increase, the model can learn increasingly complex patterns. However, adding layers or increasing the amount of hidden units does not always yield better prediction performance and can make some problems computationally intractable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Congratulations on completing the IDEaS Summer Workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
