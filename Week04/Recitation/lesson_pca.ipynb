{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be using some of our numerical packages in Python to explore a useful technique in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are going to come in handy as we work with the data.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import a data set, the wine data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# X is a 2D array \n",
    "# y is a 1D array of integer class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---INSERT CODE BELOW--- find the dimensions of the array X\n",
    "m, n = None\n",
    "\n",
    "print('X shape = ', (m,n))\n",
    "print(str(m) + ' examples, ' + str(n) + ' features/variables')\n",
    "\n",
    "#---INSERT CODE BELOW--- Find the number of classes from the array of class labels, y to finish the print statement.\n",
    "print('There are ' + str(None) + ' classes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Limited exapmles, yet a substantial set of features. Let's see what we can get from a PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General practice is to use mean subtraction, subtract the mean of each feature (columns) for each feature in each example.\n",
    "# ---INSERT CODE BELOW--- Hint: numpy has a great way of doing this using broadcasting, only use 1 line.\n",
    "X = None\n",
    "\n",
    "# ---INSERT CODE BELOW--- Perform the singular value decomposition of X, generate the rectangular diagonal S matrix from the \n",
    "# returned singular values, and generate the pricipal component scores = U * S\n",
    "def generatePCS(X):\n",
    "    m,n = X.shape\n",
    "    # svd\n",
    "    U, s, Vt = None\n",
    "    S = None\n",
    "    PCs = None\n",
    "    \n",
    "    return (PCs, s)\n",
    "\n",
    "\n",
    "from utils import PlotPC1PC2 #Loading a helper function to plot and save time\n",
    "PCs, s = generatePCS(X)\n",
    "PlotPC1PC2(PCs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to differentiate the class 1 and 2 very well... It seems these values aren't on comparable scales. Let's try to investigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---INSERT CODE BELOW--- Using numpy, print two vectors of length n showing the max and min of each of the features\n",
    "print(None)\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some of the features have very different ranges and scales, let's bring these to a more comparable level using feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again using numpy, find the standard deviation of each feature across all samples (i.e. the standard deviation of each column)\n",
    "# and divide by this value.\n",
    "# ---INSERT CODE HERE--- only use one line.\n",
    "X = None \n",
    "\n",
    "\n",
    "PCs, s = generatePCS(X)\n",
    "PlotPC1PC2(PCs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much better result, we are observing some distinct separation in the PCA space with only 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will plot the percent of variance that each component describes according to the sorted eigenvalues of X'X\n",
    "# We generate the eigenvalues from the singular values, s. What is the relationship? \n",
    "\n",
    "# ---INSERT CODE HERE--- Generate an array of the eigenvalues scaled such that their sum = 1. \n",
    "scaled_eigenvalues = None\n",
    "\n",
    "# Making the scree plot\n",
    "_ = np.arange(len(s) + 1)\n",
    "plt.plot(_, scaled_eigenvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It looks like the first few components are responsible for describing most of the dataset. Now we will try to use this for dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe_variance(scaled_eigenvalues, thresh = 0.95):\n",
    "    # Inputs:\n",
    "    #       scaled_eigenvalues - array of eigenvalues scaled such that their sum = 1\n",
    "    #       thresh - percent of variance threshold, the function returns the number of components it takes to meet this value.\n",
    "    #---INSERT CODE HERE--- Finish the function to show how many principal components it takes to describe a \n",
    "    # certain percentage of the variance in the dataset given by thresh\n",
    "    eig_sum = 0\n",
    "\n",
    "    print(str(eig_sum*100) + '% of the variance is described by ' + str(None) + ' components.')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function, use a thresh = 0.95\n",
    "describe_variance(scaled_eigenvalues, thresh = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that most of the data can be described by fewer than the original number of features. This isn't a vastly different number in this case, but can be extremely useful for datasets with a high number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an exercise left to the class, generate a reduced dimension dataset based on the number of features it takes to describe 90% of the variance.\n",
    "The data set should be returned as a m x n_reduced array.\n",
    "You will upload 2 files to the HW. This completed notebook labeled as discussed earlier in the course and a .csv file of the reduced dimension array with NO HEADERS and NO ROW LABELS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
